{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, setup a temp environment, so you don't mess up other python libraries on your system.\n",
    "More info can be found [here](https://docs.python.org/3/tutorial/venv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv langchain\n",
    "!source langchain/bin/activate \n",
    "#on windows comment out above line and uncomment next line: \n",
    "#!tutorial-env\\Scripts\\activate.bat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a baseline snapshot of the system, so we can see what the system architecture and versions are in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some important librarys, first the operating system (os) and then the ability to read the `.env` file which has secrets from `dotenv` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now check the system configuration against the baseline we set earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.4\n",
      "IPython version      : 8.14.0\n",
      "\n",
      "Compiler    : Clang 14.0.0 (clang-1400.0.29.202)\n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 10\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions -v -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment variables from `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text\n",
    "We will now start with basic strings, which is the primary construct sent to and from the LLM.  Let's set a basic string and store it into the variable `text` and print it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What day comes after Monday?\n"
     ]
    }
   ],
   "source": [
    "text = \"What day comes after Monday?\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Messages\n",
    "Chat messages are just like text, but with a specified message type.  Within Langchain, there are 3 main types of messages to LLMs (System, Human, AI).  Note, there are more, including FunctionMessage, but we will cover that later under agents.\n",
    "\n",
    "* **System** - Gives helpful background context that tell the AI what to do\n",
    "* **Human** - Sends messages that are intented to represent the user\n",
    "* **AI** - Returned messages that show what the AI responded with\n",
    "\n",
    "For more information, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction), where OpenAI calls these system, user, and assistant.  Langchain handles this translation for you within the [ChatMessage schema](https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.schema).\n",
    "\n",
    "Let's load up those libraries and also the ChatOpenAI chat_model, which is a wrapper around OpenAI Chat LLMs.  We will also instantiate ChatOpenAI with the temperature and api key).  \n",
    "\n",
    "The temperature is a floating point (decimal), representing the sampling temperature.  The values of the temperatrure, range from 0-1, where 0 is a deterministic sample, where you will get the same answer each time and 1 is a random model, where the model has more freedom to be \"creative\". \n",
    "\n",
    "The API key is passed in from the .env file and should never be included in code repositories nor should it be shared with anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets feed the chat method some messages, in this case one for the system, the other for the human.  The system message gives the model some instructions on how to behave.  The human message is the prompt to the model.  In this case, lets just feed it the `text` variable we saved above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Tuesday, because Monday likes to take things one day at a time.', additional_kwargs={})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out the answer to their question\" \n",
    "                      \"in one short sentence, however be funny in your response.\"),\n",
    "        HumanMessage(content=f\"{text}?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can see the response back from the model and it is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go back to the ChatOpenAI method above and change the temperature, to for example 0 and run it a couple times.  Then, change it to 1.0 and run it a couple times.  Congrats, you are on your way to becoming a data scientist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets make a more advance prompt, you can also pass more chat history w/ responses from the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You should go hiking, biking, and white-water rafting in the Rocky Mountains!', additional_kwargs={})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a rude AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "        HumanMessage(content=\"I like the mountains where should I go?\"),\n",
    "        AIMessage(content=\"You should go to Colorado\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there in the Summer?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
